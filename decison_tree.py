# -*- coding: utf-8 -*-
"""decison-tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZCMglvvPwg2V22xAJmhjKJMjgsel3ulE
"""

import pyspark
import csv

from pyspark import SparkContext
from pyspark.sql import SparkSession # use PySpark SQL module to import SparkSession because ML works with SparkSession
import pyspark.sql.functions as F
from pyspark.sql.types import *
from pyspark.ml import Pipeline
# the following 3 are submodels of ML 
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator

# the following code is run from my ICDS-ACI "DT" directory with a subdirectory called "decision_tree_plot"
from decision_tree_plot.decision_tree_parser import decision_tree_parse
from decision_tree_plot.decision_tree_plot import plot_trees

# this code runs Spark in local mode which is why I use SparkSession not SparkContext for the ML pipeline 
ss=SparkSession.builder.master("local").appName("lab 7 DT").getOrCreate()

# read the csv data and interpret the first row as the header with header = True
# infer the type of different columns based on their values with inferSchema = True
data = ss.read.csv("breast-cancer-wisconsin.data.txt", header=True, inferSchema=True)

# split the data into training and test subsets 
trainingData, testData= data.randomSplit([0.7, 0.3], seed=5068)

labelIndexer = StringIndexer(inputCol="class", outputCol="indexedLabel").fit(data)

bnIndexer = StringIndexer(inputCol="bare_nuclei", outputCol="bare_nuclei_index").fit(data)

input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', 'single_epith_cell_size', 'bland_chrom', 'norm_nucleoli', 'mitoses', 'bare_nuclei_index']

assembler = VectorAssembler( inputCols=input_features, outputCol="features")

labelConverter = IndexToString(inputCol="indexedLabel", outputCol="predictedClass", labels=labelIndexer.labels)

evaluate = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="f1")

dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="features")

pipeline=Pipeline(stages=[labelIndexer, bnIndexer, assembler, dt, labelConverter])

# specify the range of hyperparameters to be used for finding the best DT model 
# max depth: 2, 3, 4, 5, 6
# minInstancesPerNode: 1, 2, 3
paramGrid = ParamGridBuilder().addGrid(dt.maxDepth, [2, 3, 4, 5, 6]).addGrid(dt.minInstancesPerNode, [1, 2, 3]).build()

# specify the components of cross validator
CV = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluate, numFolds=5 )

# run cross-validation 5 fold on all hyperparamter combinations 
cvDTModel= CV.fit(data)

# list the average k-fold evaluation metric F1 scores
cvDTModel.avgMetrics
# returns: [0.93184790210825, 0.93184790210825, 0.93184790210825, 0.9399776496950529,
# 0.9442109168896071, 0.9370303661394566, 0.9432045638584471, 0.9470706942977346,
# 0.9368485842355734, 0.9445647636651946, 0.9473524856421205, 0.9374124041978098,
# 0.9411363451619698, 0.9438663719658215, 0.9356463544526311]

# connect each hyperparameter combination with its k-fold average evaluation result
lst = [pair for pair in zip(cvDTModel.avgMetrics, paramGrid)]
lst

rdd=ss.sparkContext.parallelize(lst, 1)
rdd.saveAsTextFile("F1_HyperParameters_1")

# find the nest model based on F1 score of hyperparameter combinations 
BestDT=cvDTModel.bestModel.stages[3]
BestParams=BestDT.extractParamMap()
BestParams

# BestParams is a dictionary that can be converted to a list, then an RDD
rdd2=ss.sparkContext.parallelize(BestParams.items())
rdd2.saveAsTextFile("BestParams_1")